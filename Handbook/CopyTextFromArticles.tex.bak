\chapter{Copy Text From Literature}
\section{Nystr\"{o}m Method}
Matrix Coherence and the Nystrom Method Ameet Talwalkar
    The Nystrom method is an effcient technique
    used to speed up large-scale learning applications by generating low-rank approximations.
    Crucial to the performance of this technique
    is the assumption that a matrix can be well
    approximated by working exclusively with a
    subset of its columns. In this work we relate this assumption to the concept of matrix
    coherence, connecting coherence to the performance of the Nystrom method. Making
    use of related work in the compressed sensing and the matrix completion literature, we
    derive novel coherence-based bounds for the
    Nystrom method in the low-rank setting. We
    then present empirical results that corroborate these theoretical bounds. Finally, we
    present more general empirical results for the
    full-rank setting that convincingly demonstrate the ability of matrix coherence to measure the degree to which information can be
    extracted from a subset of columns.

        Modern problems in computer vision, natural language processing, computational biology and other areas often involve datasets containing millions of training instances. However, several standard methods in
        machine learning, such as spectral clustering (Ng et
        al., 2001), manifold learning techniques (de Silva and
        Tenenbaum, 2003; Sch¨olkopf et al., 1998), kernel ridge
        regression (Saunders et al., 1998) or other kernel-based
        algorithms do not scale to such orders of magnitude. In
        fact, even storage of the matrices associated with these
        datasets can be problematic since they are often not
        sparse and hence the number of entries is extremely
        large. As shown by Williams and Seeger (2000), the
        Nystr¨om method provides an attractive solution when
        working with large-scale datasets by operating on only
        a small part of the original matrix to generate a lowrank approximation. The Nystr¨om method has been
        shown to work well in practice for various applications
        ranging from manifold learning to image segmentation (Fowlkes et al., 2004; Platt, 2004; Talwalkar et
        al., 2008; Zhang et al., 2008).
        The eﬀectiveness of the Nystr¨om method hinges on
        two key assumptions on the input matrix, G. First,
        we assume that a low-rank approximation to G can be
        eﬀective for the task at hand. This assumption is often
        true empirically as evidenced by the widespread use
        of singular value decomposition (SVD) and principal
        component analysis (PCA) in practical applications.
        As expected, the Nystr¨om method is not appropriate
        in cases where this assumption does not hold, which
        explains its poor performance in the experimental results of Fergus et al. (2009). Previous work analyzing
        the performance of the Nystr¨om method incorporates
        this low-rank assumption into theoretical guarantees
        by comparing the Nystr¨om approximation to the ‘best’
        low-rank approximation, i.e., the approximation constructed from the top singular values and singular vectors of G (see Section 2 for further discussion) (Drineas
        and Mahoney, 2005; Kumar et al., 2009a).
        The second crucial assumption of the Nystr¨om method
        involves the sampling-based nature of the algorithm,
        namely that an accurate low-rank approximation can
        be generated exclusively from information extracted
        from a small subset of l ≪ n columns of G. This
        assumption is not generally true for all matrices. 
        
Michael Mahoney - videolectures.net Statistical Leverage
        Given an m x n matrix A and a rank parameter k, define the leverage of the i-th row of A to be the i-th diagonal element of the projection matrix onto the span of the top k left singular vectors of A. In this case, "high leverage" rows have a disproportionately large amount of the "mass" in the top singular vectors. Historically, this statistical concept (and generalizations of it) has found extensive applications in diagnostic regression analysis. Recently, this concept has also been central in the development of improved randomized algorithms for several fundamental matrix problems that have broad applications in machine learning and data analysis. Two examples of the use of statistical leverage for improved worst-case analysis of matrix algorithms will be described. The first problem is the least squares approximation problem, in which there are n constraints and d variables. Classical algorithms, dating back to Gauss and Legendre, use O(nd2) time. We describe a randomized algorithm that uses only O(n d log d) time to compute a relative-error, i.e., 1+/-epsilon, approximation. The second problem is the problem of selecting a "good" set of exactly k columns from an m x n matrix, and the algorithm of Gu and Eisenstat provides the best previously existing result. We describe a two-stage algorithm that improves on their result. Recent applications of statistical leverage ideas in modern large-scale machine learning and data analysis will also be briefly described. This concept has proven to be particularly fruitful in large data applications where modeling decisions regarding what computations to perform are made for computational reasons, as opposed to having any realistic hope that the statistical assumptions implicit in those computations are satisfied by the data. 